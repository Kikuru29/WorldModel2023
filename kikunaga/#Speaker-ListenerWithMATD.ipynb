{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4488f4",
   "metadata": {},
   "source": [
    "# AgileRL Speaker-Listener with MATD3\n",
    "https://docs.agilerl.com/en/latest/tutorials/pettingzoo/matd3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b039f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (23.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2e6aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pettingzoo[mpe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: agilerl in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (0.1.19)\n",
      "Requirement already satisfied: SuperSuit<4.0.0,>=3.9.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (3.9.1)\n",
      "Requirement already satisfied: accelerate<0.19.0,>=0.18.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (0.18.0)\n",
      "Requirement already satisfied: dill<0.4.0,>=0.3.7 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (0.3.7)\n",
      "Requirement already satisfied: fastrand<2.0.0,>=1.3.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (1.8.0)\n",
      "Requirement already satisfied: flatten_dict<0.5.0,>=0.4.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (0.4.2)\n",
      "Requirement already satisfied: gymnasium<0.29.0,>=0.28.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (0.28.1)\n",
      "Requirement already satisfied: h5py<4.0.0,>=3.8.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (3.9.0)\n",
      "Requirement already satisfied: hydra-core<2.0.0,>=1.3.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (1.3.2)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.4.3 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (3.8.0)\n",
      "Requirement already satisfied: minari<0.5.0,>=0.4.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (0.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (1.26.2)\n",
      "Requirement already satisfied: omegaconf<3.0.0,>=2.3.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (2.3.0)\n",
      "Requirement already satisfied: pettingzoo<2.0.0,>=1.23.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (1.24.2)\n",
      "Requirement already satisfied: pre-commit<4.0.0,>=3.4.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (3.6.0)\n",
      "Requirement already satisfied: redis<5.0.0,>=4.4.4 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (4.6.0)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (1.1.0)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (2.3.0.dev20231214)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (4.66.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.30.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (4.36.1)\n",
      "Requirement already satisfied: wandb<0.14.0,>=0.13.10 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from agilerl) (0.13.11)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from accelerate<0.19.0,>=0.18.0->agilerl) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from accelerate<0.19.0,>=0.18.0->agilerl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from accelerate<0.19.0,>=0.18.0->agilerl) (6.0.1)\n",
      "Requirement already satisfied: six<2.0,>=1.12 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from flatten_dict<0.5.0,>=0.4.2->agilerl) (1.16.0)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from gymnasium<0.29.0,>=0.28.1->agilerl) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from gymnasium<0.29.0,>=0.28.1->agilerl) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from gymnasium<0.29.0,>=0.28.1->agilerl) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from gymnasium<0.29.0,>=0.28.1->agilerl) (0.0.4)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from hydra-core<2.0.0,>=1.3.2->agilerl) (4.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.4.3->agilerl) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-storage==2.5.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from minari<0.5.0,>=0.4.1->agilerl) (2.5.0)\n",
      "Requirement already satisfied: typer==0.9.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (0.9.0)\n",
      "Requirement already satisfied: portion==2.4.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from minari<0.5.0,>=0.4.1->agilerl) (2.4.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2.22.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2.15.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2.31.0)\n",
      "Requirement already satisfied: sortedcontainers~=2.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from portion==2.4.0->minari<0.5.0,>=0.4.1->agilerl) (2.4.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from typer==0.9.0->typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (13.7.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from pre-commit<4.0.0,>=3.4.0->agilerl) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from pre-commit<4.0.0,>=3.4.0->agilerl) (2.5.33)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from pre-commit<4.0.0,>=3.4.0->agilerl) (1.8.0)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from pre-commit<4.0.0,>=3.4.0->agilerl) (20.25.0)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from SuperSuit<4.0.0,>=3.9.0->agilerl) (1.2.7)\n",
      "Requirement already satisfied: filelock in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from torch<3.0.0,>=2.0.1->agilerl) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from torch<3.0.0,>=2.0.1->agilerl) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from torch<3.0.0,>=2.0.1->agilerl) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from torch<3.0.0,>=2.0.1->agilerl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from torch<3.0.0,>=2.0.1->agilerl) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from transformers<5.0.0,>=4.30.0->agilerl) (0.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from transformers<5.0.0,>=4.30.0->agilerl) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from transformers<5.0.0,>=4.30.0->agilerl) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from transformers<5.0.0,>=4.30.0->agilerl) (0.4.1)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (3.1.40)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (1.39.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (69.0.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from wandb<0.14.0,>=0.13.10->agilerl) (3.20.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb<0.14.0,>=0.13.10->agilerl) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (2023.11.17)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from virtualenv>=20.10.0->pre-commit<4.0.0,>=3.4.0->agilerl) (0.3.8)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from virtualenv>=20.10.0->pre-commit<4.0.0,>=3.4.0->agilerl) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from jinja2->torch<3.0.0,>=2.0.1->agilerl) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from sympy->torch<3.0.0,>=2.0.1->agilerl) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb<0.14.0,>=0.13.10->agilerl) (5.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (4.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (1.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]==0.9.0->minari<0.5.0,>=0.4.1->agilerl) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage==2.5.0->minari<0.5.0,>=0.4.1->agilerl) (0.4.8)\n",
      "Requirement already satisfied: imageio in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from imageio) (1.26.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages (from imageio) (10.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo[mpe]\n",
    "!pip install agilerl\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7620cb",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This tutorial shows how to train an MATD3 agent on the simple speaker listener multi-particle environment.\n",
    "\n",
    "Authors: Michael (https://github.com/mikepratt1), Nickua (https://github.com/nicku-a)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2af91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15224e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb669d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network configuration\n",
    "def define_network_config():\n",
    "    return {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Actor hidden size\n",
    "    }\n",
    "\n",
    "# Define the initial hyperparameters\n",
    "def initialize_hyperparameters():\n",
    "    return {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MATD3\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "        # Instantiate a tournament selection object (used for HPO)\n",
    "        'TOURNAMENT_SIZE': 2,\n",
    "        'ELITISM': True,\n",
    "        # Instantiate a mutations object (used for HPO)\n",
    "        'NO_MUTATION': 0.2,\n",
    "        'ARCHITECTURE_MUTATION': 0.2,\n",
    "        'NEW_LAYER_MUTATION': 0.2,\n",
    "        'PARAMETER_MUTATION': 0.2,\n",
    "        'ACTIVATION_MUTATION': 0,\n",
    "        'RL_HP_MUTATION': 0.2,\n",
    "        'RL_HP_SELECTION': [\"lr\", \"learn_step\", \"batch_size\"], # RL hyperparams selected for mutation\n",
    "        'MUTATION_SD': 0.1,\n",
    "\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102b54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple speaker listener environment as a parallel environment\n",
    "def initialize_environment():\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "# Configure the multi-agent algo input arguments\n",
    "def set_action_and_state_dimensions(env, init_hp):\n",
    "    \"\"\"\n",
    "    環境から行動次元と状態次元を設定し、初期ハイパーパラメータを更新する。\n",
    "    env: 学習環境\n",
    "    init_hp: 初期ハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # まず、状態次元を設定する\n",
    "        # 状態空間が離散的か連続的かに基づいて状態次元を取得する\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        # 連続的な状態空間の場合\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "\n",
    "    try:\n",
    "        # 次に、行動次元を設定する\n",
    "        # 行動空間が離散的か連続的かに基づいて行動次元を取得する\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        init_hp[\"DISCRETE_ACTIONS\"] = True\n",
    "        init_hp[\"MAX_ACTION\"] = None\n",
    "        init_hp[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        # 連続的な行動空間の場合\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        init_hp[\"DISCRETE_ACTIONS\"] = False\n",
    "        init_hp[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        init_hp[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # 状態次元の調整（CHANNELS_LAST オプションが True の場合）\n",
    "    if init_hp[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    return state_dim, action_dim, init_hp, one_hot\n",
    "\n",
    "\n",
    "def list_initial_population(algo, state_dim, action_dim, one_hot, net_config, init_hp, population_size, device):\n",
    "    \"\"\"\n",
    "    初期人口を生成する。\n",
    "    algo: 使用する強化学習アルゴリズム\n",
    "    state_dim: 状態次元\n",
    "    action_dim: 行動次元\n",
    "    one_hot: 状態がワンホットエンコードされているかどうか\n",
    "    net_config: ネットワーク構成\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    device: 使用するデバイス（例: \"cuda\"、\"mps\"、\"cpu\"）\n",
    "    \"\"\"\n",
    "    pop_list = []\n",
    "    for _ in range(init_hp[\"POPULATION_SIZE\"]):\n",
    "        agent = initialPopulation(algo, state_dim, action_dim, one_hot, net_config, init_hp, population_size, device)\n",
    "        pop_list.append(agent)\n",
    "\n",
    "    return pop_list\n",
    "\n",
    "def create_initial_population(algo, state_dim, action_dim, one_hot, net_config, init_hp, population_size, device):\n",
    "    pop = initialPopulation(\n",
    "        init_hp[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        net_config,\n",
    "        init_hp,\n",
    "        population_size=init_hp[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "    if pop is None:\n",
    "        return []\n",
    "    else:\n",
    "        return pop\n",
    "\n",
    "\n",
    "def configure_replay_buffer(init_hp, field_names, device):\n",
    "    \"\"\"\n",
    "    リプレイバッファを設定する。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    agent_ids: エージェントのIDリスト\n",
    "    device: 使用するデバイス（例: \"cuda\"、\"mps\"、\"cpu\"）\n",
    "    \"\"\"\n",
    "    # リプレイバッファを格納するためのデータ構造を定義\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "\n",
    "    # リプレイバッファのインスタンスを作成\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        init_hp[\"MEMORY_SIZE\"],  # バッファの最大サイズ\n",
    "        field_names=field_names,  # 格納するフィールド名\n",
    "        agent_ids=init_hp[\"AGENT_IDS\"],      # エージェントのID\n",
    "        device=device,             # 使用するデバイス\n",
    "    )\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "def tournament_selection(init_hp):\n",
    "    \"\"\"\n",
    "    トーナメント選択の設定を行う。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    \"\"\"\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=init_hp['TOURNAMENT_SIZE'],\n",
    "        elitism=init_hp['ELITISM'],\n",
    "        population_size=init_hp['POPULATION_SIZE'],\n",
    "        evo_step =1,\n",
    "    )\n",
    "    return tournament\n",
    "\n",
    "\n",
    "def mutations_config(init_hp, net_config):\n",
    "    \"\"\"\n",
    "    突然変異の設定を行う。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    net_config: ネットワーク構成\n",
    "    \"\"\"\n",
    "    mutations = Mutations(\n",
    "        algo=init_hp[\"ALGO\"],\n",
    "        no_mutation=init_hp['NO_MUTATION'],\n",
    "        architecture=init_hp['ARCHITECTURE_MUTATION'],\n",
    "        new_layer_prob=init_hp['NEW_LAYER_MUTATION'],\n",
    "        parameters=init_hp['PARAMETER_MUTATION'],\n",
    "        activation=init_hp['ACTIVATION_MUTATION'],\n",
    "        rl_hp=init_hp['RL_HP_MUTATION'],\n",
    "        rl_hp_selection=init_hp['RL_HP_SELECTION'],\n",
    "        mutation_sd=init_hp['MUTATION_SD'],\n",
    "        agent_ids=init_hp[\"AGENT_IDS\"],\n",
    "        arch=net_config[\"arch\"],\n",
    "        rand_seed=1,\n",
    "        device=device\n",
    "    )\n",
    "    return mutations\n",
    "\n",
    "\n",
    "def save_trained_model(model, path, filename):\n",
    "    \"\"\"\n",
    "    model: 学習済みのモデル\n",
    "    path: モデルを保存するディレクトリのパス\n",
    "    filename: 保存するファイルの名前\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(model.state_dict(), os.path.join(path, filename))\n",
    "\n",
    "\n",
    "def training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes, max_steps):\n",
    "    epsilon = 1.0\n",
    "    eps_end = 0.1\n",
    "    eps_decay = 0.995\n",
    "    evo_epochs = 20\n",
    "    evo_loop = 1\n",
    "    elite = None\n",
    "\n",
    "    if pop is not None and len(pop) > 0:\n",
    "        elite = pop[0]\n",
    "\n",
    "    for idx_epi in range(max_episodes):\n",
    "        for agent in pop:\n",
    "            state, info = env.reset()\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if init_hp[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info.get(\"agent_mask\")\n",
    "                env_defined_actions = info.get(\"env_defined_actions\")\n",
    "\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                action = discrete_action if agent.discrete_actions else cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "                if init_hp[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                if (memory.counter % agent.learn_step == 0) and (len(memory) >= agent.batch_size):\n",
    "                    experiences = memory.sample(agent.batch_size)\n",
    "                    agent.learn(experiences)\n",
    "\n",
    "                if init_hp[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    swap_channels=init_hp[\"CHANNELS_LAST\"],\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in pop\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in pop]}'\n",
    "            )\n",
    "\n",
    "            if len(pop) > 0:\n",
    "                elite, pop = tournament.select(pop)\n",
    "                pop = mutations.mutation(pop)\n",
    "\n",
    "    if elite is not None:\n",
    "        save_trained_model(elite, './models/MATD3', \"MATD3_trained_agent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09eeba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== AgileRL Online Multi-Agent Demo =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/#Speaker-ListenerWithMATD.ipynb セル 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tournament \u001b[39m=\u001b[39m tournament_selection(init_hp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m mutations \u001b[39m=\u001b[39m mutations_config(init_hp, net_config)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes\u001b[39m=\u001b[39;49m\u001b[39m6000\u001b[39;49m, max_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m save_trained_model(pop[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m./models/MATD3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMATD3_trained_agent.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/#Speaker-ListenerWithMATD.ipynb セル 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m \u001b[39mif\u001b[39;00m (memory\u001b[39m.\u001b[39mcounter \u001b[39m%\u001b[39m agent\u001b[39m.\u001b[39mlearn_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(memory) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     experiences \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample(agent\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mlearn(experiences)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m \u001b[39mif\u001b[39;00m init_hp[\u001b[39m\"\u001b[39m\u001b[39mCHANNELS_LAST\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m     next_state \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m         agent_id: np\u001b[39m.\u001b[39mexpand_dims(ns, \u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m         \u001b[39mfor\u001b[39;00m agent_id, ns \u001b[39min\u001b[39;00m next_state\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kikunagarikuto/Documents/WorldModel2023/kikunaga/%23Speaker-ListenerWithMATD.ipynb#X11sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/agilerl/algorithms/matd3.py:722\u001b[0m, in \u001b[0;36mMATD3.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m             actor_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 722\u001b[0m         actor_optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    724\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscores) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    725\u001b[0m     \u001b[39mfor\u001b[39;00m (\n\u001b[1;32m    726\u001b[0m         actor,\n\u001b[1;32m    727\u001b[0m         actor_target,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_targets_2,\n\u001b[1;32m    739\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/optim/adam.py:382\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    380\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mis_complex(param):\n\u001b[1;32m    383\u001b[0m     grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(grad)\n\u001b[1;32m    384\u001b[0m     exp_avg \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    #device = torch.device(\"mps\")\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "\n",
    "    net_config = define_network_config()\n",
    "    init_hp = initialize_hyperparameters()\n",
    "    env = initialize_environment()\n",
    "\n",
    "    # Set the number of agents in the INIT_HP dictionary\n",
    "    init_hp[\"N_AGENTS\"] = env.num_agents  # Assuming env.agents gives the list of agents\n",
    "    init_hp[\"AGENT_IDS\"] = env.agents  # エージェントIDのリストを設定\n",
    "\n",
    "\n",
    "    state_dim, action_dim, init_hp, one_hot= set_action_and_state_dimensions(env, init_hp)\n",
    "    pop = create_initial_population(init_hp[\"ALGO\"], state_dim, action_dim, one_hot, net_config, init_hp, init_hp[\"POPULATION_SIZE\"], device)\n",
    "    memory = configure_replay_buffer(init_hp, env.agents, device=device)\n",
    "    tournament = tournament_selection(init_hp)\n",
    "    mutations = mutations_config(init_hp, net_config)\n",
    "    \n",
    "    training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes=6000, max_steps=25)\n",
    "    \n",
    "    save_trained_model(pop[0], \"./models/MATD3\", \"MATD3_trained_agent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4757375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Episode: 0 ---------------\n",
      "Episodic Reward:  -20.273208225947414\n",
      "speaker_0 reward: -10.136604112973707\n",
      "listener_0 reward: -10.136604112973707\n",
      "--------------- Episode: 1 ---------------\n",
      "Episodic Reward:  -11.66051896556302\n",
      "speaker_0 reward: -5.83025948278151\n",
      "listener_0 reward: -5.83025948278151\n",
      "--------------- Episode: 2 ---------------\n",
      "Episodic Reward:  -73.67722621533407\n",
      "speaker_0 reward: -36.83861310766704\n",
      "listener_0 reward: -36.83861310766704\n",
      "--------------- Episode: 3 ---------------\n",
      "Episodic Reward:  -43.667201403379316\n",
      "speaker_0 reward: -21.833600701689658\n",
      "listener_0 reward: -21.833600701689658\n",
      "--------------- Episode: 4 ---------------\n",
      "Episodic Reward:  -15.930634573818322\n",
      "speaker_0 reward: -7.965317286909161\n",
      "listener_0 reward: -7.965317286909161\n",
      "--------------- Episode: 5 ---------------\n",
      "Episodic Reward:  -77.70489811880245\n",
      "speaker_0 reward: -38.85244905940122\n",
      "listener_0 reward: -38.85244905940122\n",
      "--------------- Episode: 6 ---------------\n",
      "Episodic Reward:  -24.27136308952488\n",
      "speaker_0 reward: -12.13568154476244\n",
      "listener_0 reward: -12.13568154476244\n",
      "--------------- Episode: 7 ---------------\n",
      "Episodic Reward:  -24.16418039964721\n",
      "speaker_0 reward: -12.082090199823606\n",
      "listener_0 reward: -12.082090199823606\n",
      "--------------- Episode: 8 ---------------\n",
      "Episodic Reward:  -20.97078882951033\n",
      "speaker_0 reward: -10.485394414755165\n",
      "listener_0 reward: -10.485394414755165\n",
      "--------------- Episode: 9 ---------------\n",
      "Episodic Reward:  -41.10477409673252\n",
      "speaker_0 reward: -20.55238704836626\n",
      "listener_0 reward: -20.55238704836626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "\n",
    "# Define function to return image\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm into the MADDPG object\n",
    "    path = \"./models/MATD3/MATD3_trained_agent.pt\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 10  # Number of episodes to test agent on\n",
    "    max_steps = 25  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()\n",
    "\n",
    "    # Save the gif to specified path\n",
    "    gif_path = \"./videos/\"\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        os.path.join(\"./videos/\", \"speaker_listener.gif\"), frames, duration=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
