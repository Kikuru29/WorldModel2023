{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4488f4",
   "metadata": {},
   "source": [
    "# AgileRL Speaker-Listener with MATD3\n",
    "https://docs.agilerl.com/en/latest/tutorials/pettingzoo/matd3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b039f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfadb438-9362-4b9d-9d05-ae54b857ffcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python3 -m pip install playsound\n",
    "#!python3 -m pip install PyObjC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2e6aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pettingzoo[mpe]\n",
    "#!pip install agilerl\n",
    "#!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This tutorial shows how to train an MATD3 agent on the simple speaker listener multi-particle environment.\n",
    "\n",
    "Authors: Michael (https://github.com/mikepratt1), Nickua (https://github.com/nicku-a)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from tqdm import trange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "\n",
    "# log setting\n",
    "logging.basicConfig(level=logging.INFO, filename='matd3_taining.logs', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#コマンドライン引数の設定\n",
    "parser = argparse.ArgumentParser(description=\"Train a MATD3 agent on a multi-particle environment\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=5000, help=\"Number of training epochs\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #現在日時を取得\n",
    "    dt_now = datetime.datetime.now()\n",
    "    str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "    print(str_dt_now)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"mps\")\n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "\n",
    "    # Define the network configuration\n",
    "    # ネットワークコンフィグレーションの定義\n",
    "    NET_CONFIG = {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Actor hidden size\n",
    "    }\n",
    "\n",
    "    # Define the initial hyperparameters\n",
    "    # 初期ハイパーパラメータの定義\n",
    "    INIT_HP = {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MATD3\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "    }\n",
    "\n",
    "    # Define the simple speaker listener environment as a parallel environment\n",
    "    # シンプル・スピーカー・リスナー環境（並列）の定義\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the multi-agent algo input arguments\n",
    "    # マルチ・エージェントアルゴへの入力引数の設定\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    \n",
    "    if False: # デバッグ出力\n",
    "        print(\"state_dim\", state_dim)\n",
    "        print(\"one_hot\", one_hot)\n",
    "        print( 'INIT_HP[\"DISCRETE_ACTIONS\"]', INIT_HP[\"DISCRETE_ACTIONS\"])\n",
    "        print( 'INIT_HP[\"MAX_ACTION\"]', INIT_HP[\"MAX_ACTION\"])\n",
    "        print( 'INIT_HP[\"MIN_ACTION\"]', INIT_HP[\"MIN_ACTION\"])\n",
    "    \n",
    "\n",
    "    # Not applicable to MPE environments, used when images are used for observations (Atari environments)\n",
    "    # MPE環境には適用されない。観測のために画像が使用される場合に使う（アタリ環境）\n",
    "    if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    # エージェントとエージェントIDの数を、ハイパーパラメータディクショナリの初期化のために加える\n",
    "    INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "    INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "    \n",
    "    if True: # デバッグ出力\n",
    "        print('state_dim', state_dim)\n",
    "        print('action_dim', action_dim)\n",
    "        print('one_hot', one_hot)\n",
    "        print('NET_CONFIG')\n",
    "        pprint.pprint(NET_CONFIG)\n",
    "        print('INIT_HP')\n",
    "        pprint.pprint(INIT_HP)\n",
    "        print('device', device)\n",
    "    \n",
    "    # Create a population ready for evolutionary hyper-parameter optimisation\n",
    "    # 進化的なハイパーパラメータ最適化のための母集団を作成する\n",
    "    population = initialPopulation(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Configure the multi-agent replay buffer\n",
    "    # マルチ・エージェント・リプレイバッファを設定する\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        INIT_HP[\"MEMORY_SIZE\"],\n",
    "        field_names=field_names,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Instantiate a tournament selection object (used for HPO)\n",
    "    # トーナメント選択オブジェクトのインスタンス化（HPOで使用）\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=2,  # Tournament selection size\n",
    "        elitism=True,  # Elitism in tournament selection\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "        evo_step=1,\n",
    "    )  # Evaluate using last N fitness scores\n",
    "\n",
    "    # Instantiate a mutations object (used for HPO)\n",
    "    # ミューテーション・オブジェクトのインスタンス化（HPOで使用）\n",
    "    mutations = Mutations(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        no_mutation=0.2,  # Probability of no mutation\n",
    "        architecture=0.2,  # Probability of architecture mutation\n",
    "        new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "        parameters=0.2,  # Probability of parameter mutation\n",
    "        activation=0,  # Probability of activation function mutation\n",
    "        rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "        rl_hp_selection=[\n",
    "            \"lr\",\n",
    "            \"learn_step\",\n",
    "            \"batch_size\",\n",
    "        ],  # RL hyperparams selected for mutation\n",
    "        mutation_sd=0.1,  # Mutation strength\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        arch=NET_CONFIG[\"arch\"],\n",
    "        rand_seed=2,#1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Define training loop parameters\n",
    "    # 学習ループ・パラメータを定義\n",
    "    max_episodes = 5000 #500  # Total episodes (default: 6000)\n",
    "    max_steps = 100 #25  # Maximum steps to take in each episode\n",
    "    epsilon = 1.0  # Starting epsilon value\n",
    "    eps_end = 0.1  # Final epsilon value\n",
    "    eps_decay = 0.995  # Epsilon decay\n",
    "    evo_epochs = 20  # Evolution frequency\n",
    "    evo_loop = 1  # Number of evaluation episodes\n",
    "    elite = population[0]  # Assign a placeholder \"elite\" agent\n",
    "\n",
    "    # TensorBoardライターの設定\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training loop\n",
    "    # 学習ループ\n",
    "    for idx_epi in trange(max_episodes):\n",
    "        \n",
    "        for agent in population:  # Loop through population\n",
    "            \n",
    "            state, info = env.reset()  # Reset environment at start of episode\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "                env_defined_actions = (\n",
    "                    info[\"env_defined_actions\"]\n",
    "                    if \"env_defined_actions\" in info.keys()\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "                # Get next action from agent\n",
    "                # エージェントから次の行動を取得する\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                if agent.discrete_actions:\n",
    "                    action = discrete_action\n",
    "                else:\n",
    "                    action = cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(\n",
    "                    action\n",
    "                )  # Act in environment\n",
    "\n",
    "                # Image processing if necessary for the environment\n",
    "                # 環境に応じた画像処理を行う\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                # Save experiences to replay buffer\n",
    "                # 経験をリプレイバッファに保存する\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                # Collect the reward\n",
    "                # 報酬を受け取る\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                # Learn according to learning frequency\n",
    "                # 学習周期に合わせて学習する\n",
    "                if (memory.counter % agent.learn_step == 0) and (\n",
    "                    len(memory) >= agent.batch_size\n",
    "                ):\n",
    "                    experiences = memory.sample(\n",
    "                        agent.batch_size\n",
    "                    )  # Sample replay buffer\n",
    "                    agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "                # Update the state\n",
    "                # 状態を更新する\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                # Stop episode if any agents have terminated\n",
    "                # いずれかのエージェントが終了したならば、エピソードを停止する\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            # Save the total episode reward\n",
    "            # エピソードの合計報酬を保存する\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        # Update epsilon for exploration\n",
    "        # 探索用のイプシロンを更新\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        # Now evolve population if necessary\n",
    "        # 必要であれば、母集団を進化させる\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            # Evaluate population\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in population\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in population]}'\n",
    "            )\n",
    "\n",
    "            # Tournament selection and population mutation\n",
    "            # トーナメント選択と母集団の変異\n",
    "            elite, population = tournament.select(population)\n",
    "            population = mutations.mutation(population)\n",
    "\n",
    "    # Save the trained algorithm\n",
    "    # 学習アルゴリズムを保存する\n",
    "    path = \"./models/MATD3\"\n",
    "    #path = \"./\"+str_dt_now+\"/models/MATD3\"\n",
    "    filename = \"MATD3_trained_agent.pt\"\n",
    "    filename = f\"MATD3_trained_agent_{str_dt_now}.pt\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_path = os.path.join(path, filename)\n",
    "    elite.saveCheckpoint(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4757375",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m matd3 \u001b[38;5;241m=\u001b[39m MATD3(\n\u001b[1;32m     62\u001b[0m     state_dim,\n\u001b[1;32m     63\u001b[0m     action_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Load the saved algorithm into the MADDPG object\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#path = \"./models/MATD3/MATD3_trained_agent.pt\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#path = \"./\"+str_dt_now+\"/models/MATD3/MATD3_trained_agent.pt\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/MATD3/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m matd3\u001b[38;5;241m.\u001b[39mloadCheckpoint(path)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Define test loop parameters\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "\n",
    "# Define function to return image\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm into the MADDPG object\n",
    "    #path = \"./models/MATD3/MATD3_trained_agent.pt\"\n",
    "    #path = \"./\"+str_dt_now+\"/models/MATD3/MATD3_trained_agent.pt\"\n",
    "    path = f\"./models/MATD3/{filename}\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 10  # Number of episodes to test agent on\n",
    "    max_steps = 25  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "    \n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()\n",
    "\n",
    "    # Save the gif to specified path\n",
    "    gif_path = \"./videos/\"\n",
    "    print(os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"))\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        #os.path.join(\"./videos/\", \"speaker_listener.gif\"), frames, duration=10\n",
    "        os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d75d986-de09-4449-ab01-394fcb3ce289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Episode: 0 ---------------\n",
      "Episodic Reward:  -28.342841043864652\n",
      "speaker_0 reward: -14.171420521932326\n",
      "listener_0 reward: -14.171420521932326\n",
      "--------------- Episode: 1 ---------------\n",
      "Episodic Reward:  -5.052549604459192\n",
      "speaker_0 reward: -2.526274802229596\n",
      "listener_0 reward: -2.526274802229596\n",
      "--------------- Episode: 2 ---------------\n",
      "Episodic Reward:  -63.22651006114474\n",
      "speaker_0 reward: -31.61325503057237\n",
      "listener_0 reward: -31.61325503057237\n",
      "--------------- Episode: 3 ---------------\n",
      "Episodic Reward:  -6.109913786501052\n",
      "speaker_0 reward: -3.054956893250526\n",
      "listener_0 reward: -3.054956893250526\n",
      "--------------- Episode: 4 ---------------\n",
      "Episodic Reward:  -92.23558269633988\n",
      "speaker_0 reward: -46.11779134816994\n",
      "listener_0 reward: -46.11779134816994\n",
      "--------------- Episode: 5 ---------------\n",
      "Episodic Reward:  -14.042350239495809\n",
      "speaker_0 reward: -7.021175119747904\n",
      "listener_0 reward: -7.021175119747904\n",
      "--------------- Episode: 6 ---------------\n",
      "Episodic Reward:  -13.70970188740873\n",
      "speaker_0 reward: -6.854850943704365\n",
      "listener_0 reward: -6.854850943704365\n",
      "--------------- Episode: 7 ---------------\n",
      "Episodic Reward:  -19.16862320091758\n",
      "speaker_0 reward: -9.58431160045879\n",
      "listener_0 reward: -9.58431160045879\n",
      "--------------- Episode: 8 ---------------\n",
      "Episodic Reward:  -27.690830781259358\n",
      "speaker_0 reward: -13.845415390629679\n",
      "listener_0 reward: -13.845415390629679\n",
      "--------------- Episode: 9 ---------------\n",
      "Episodic Reward:  -58.622663525350006\n",
      "speaker_0 reward: -29.311331762675003\n",
      "listener_0 reward: -29.311331762675003\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'str_dt_now' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m gif_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./videos/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(gif_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    155\u001b[0m imageio\u001b[38;5;241m.\u001b[39mmimwrite(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m#os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10)\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gif_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeaker_listener_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_dt_now\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m), frames, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Plotting rewards\u001b[39;00m\n\u001b[1;32m    161\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'str_dt_now' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "# Define function to return image with episode number label\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"mps\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm\n",
    "    #path = f\"./models/MATD3/{filename}\"\n",
    "    path = \"/kikunaga_workspace/kikunaga/WorldModel2023/models/MATD3/MATD3_trained_agent_20231225-2315.pt\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    episodes = 10  # Number of episodes for testing\n",
    "    max_steps = 25  # Max steps per episode\n",
    "\n",
    "    rewards = []  # Total episodic rewards\n",
    "    frames = []  # Frames for visualization\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Individual agent rewards\n",
    "    \n",
    "    Episodic_rewards = []\n",
    "    #speaker_0_rewards = []  # Rewards for speaker_0\n",
    "    #listener_0_rewards = []  # Rewards for listener_0\n",
    "\n",
    "    # Test loop\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # ... [existing action selection and environment interaction code] ...\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # ... [existing environment step and reward processing code] ...\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "            \n",
    "        # Store rewards for each agent\n",
    "        Episodic_rewards.append(indi_agent_rewards['speaker_0'][-1])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save the visualization\n",
    "    dt_now = datetime.datetime.now()\n",
    "    #str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "    atr_dt_now = 20231225-2315\n",
    "    ##\n",
    "    gif_path = \"./videos/\"\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        #os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10)\n",
    "        os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10)\n",
    "\n",
    "\n",
    "    # Plotting rewards\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(Episodic_rewards, label='Episodic_rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Episodic Rewards for Speaker and Listener Agents')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642b609-f427-4afe-afae-34029d8fa55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
