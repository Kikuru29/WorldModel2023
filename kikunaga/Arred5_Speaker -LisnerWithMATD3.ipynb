{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pettingzoo[mpe]\n",
    "#!pip install agilerl\n",
    "#!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "\n",
    "\n",
    "###\n",
    "#import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "\n",
    "def get_current_datetime():\n",
    "    # 現在の日時を取得\n",
    "    dt_now = datetime.datetime.now()\n",
    "\n",
    "    # 日時を指定された形式の文字列に変換\n",
    "    str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "    return str_dt_now\n",
    "\n",
    "# この関数を呼び出して現在の日時を取得する例\n",
    "# current_datetime = get_current_datetime()\n",
    "# print(current_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network configuration\n",
    "def define_network_config():\n",
    "    return {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Actor hidden size\n",
    "    }\n",
    "\n",
    "# Define the initial hyperparameters\n",
    "def initialize_hyperparameters():\n",
    "    return {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MATD3\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "        # Instantiate a tournament selection object (used for HPO)\n",
    "        'TOURNAMENT_SIZE': 2,\n",
    "        'ELITISM': True,\n",
    "        # Instantiate a mutations object (used for HPO)\n",
    "        'NO_MUTATION': 0.2,\n",
    "        'ARCHITECTURE_MUTATION': 0.2,\n",
    "        'NEW_LAYER_MUTATION': 0.2,\n",
    "        'PARAMETER_MUTATION': 0.2,\n",
    "        'ACTIVATION_MUTATION': 0,\n",
    "        'RL_HP_MUTATION': 0.2,\n",
    "        'RL_HP_SELECTION': [\"lr\", \"learn_step\", \"batch_size\"], # RL hyperparams selected for mutation\n",
    "        'MUTATION_SD': 0.1,\n",
    "\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple speaker listener environment as a parallel environment\n",
    "def initialize_environment():\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "# Configure the multi-agent algo input arguments\n",
    "def set_action_and_state_dimensions(env, init_hp):\n",
    "    \"\"\"\n",
    "    環境から行動次元と状態次元を設定し、初期ハイパーパラメータを更新する。\n",
    "    env: 学習環境\n",
    "    init_hp: 初期ハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # まず、状態次元を設定する\n",
    "        # 状態空間が離散的か連続的かに基づいて状態次元を取得する\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        # 連続的な状態空間の場合\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "\n",
    "    try:\n",
    "        # 次に、行動次元を設定する\n",
    "        # 行動空間が離散的か連続的かに基づいて行動次元を取得する\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        init_hp[\"DISCRETE_ACTIONS\"] = True\n",
    "        init_hp[\"MAX_ACTION\"] = None\n",
    "        init_hp[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        # 連続的な行動空間の場合\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        init_hp[\"DISCRETE_ACTIONS\"] = False\n",
    "        init_hp[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        init_hp[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # 状態次元の調整（CHANNELS_LAST オプションが True の場合）\n",
    "    if init_hp[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    return state_dim, action_dim, init_hp, one_hot\n",
    "\n",
    "\n",
    "def create_initial_population(algo, state_dim, action_dim, one_hot, net_config, init_hp, population_size, device):\n",
    "    \"\"\"\n",
    "    初期人口を生成する。\n",
    "    algo: 使用する強化学習アルゴリズム\n",
    "    state_dim: 状態次元\n",
    "    action_dim: 行動次元\n",
    "    one_hot: 状態がワンホットエンコードされているかどうか\n",
    "    net_config: ネットワーク構成\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    device: 使用するデバイス（例: \"cuda\"、\"mps\"、\"cpu\"）\n",
    "    \"\"\"\n",
    "    pop = initialPopulation(\n",
    "        init_hp[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        net_config,\n",
    "        init_hp,\n",
    "        population_size=init_hp[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "    if pop is None:\n",
    "        return []\n",
    "    else:\n",
    "        return pop\n",
    "\n",
    "\n",
    "def configure_replay_buffer(init_hp, field_names, device):\n",
    "    \"\"\"\n",
    "    リプレイバッファを設定する。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    agent_ids: エージェントのIDリスト\n",
    "    device: 使用するデバイス（例: \"cuda\"、\"mps\"、\"cpu\"）\n",
    "    \"\"\"\n",
    "    # リプレイバッファを格納するためのデータ構造を定義\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "\n",
    "    # リプレイバッファのインスタンスを作成\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        init_hp[\"MEMORY_SIZE\"],  # バッファの最大サイズ\n",
    "        field_names=field_names,  # 格納するフィールド名\n",
    "        agent_ids=init_hp[\"AGENT_IDS\"],      # エージェントのID\n",
    "        device=device,             # 使用するデバイス\n",
    "    )\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "def tournament_selection(init_hp):\n",
    "    \"\"\"\n",
    "    トーナメント選択の設定を行う。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    \"\"\"\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=init_hp['TOURNAMENT_SIZE'],\n",
    "        elitism=init_hp['ELITISM'],\n",
    "        population_size=init_hp['POPULATION_SIZE'],\n",
    "        evo_step =1,\n",
    "    )\n",
    "    return tournament\n",
    "\n",
    "\n",
    "def mutations_config(init_hp, net_config):\n",
    "    \"\"\"\n",
    "    突然変異の設定を行う。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    net_config: ネットワーク構成\n",
    "    \"\"\"\n",
    "    mutations = Mutations(\n",
    "        algo=init_hp[\"ALGO\"],\n",
    "        no_mutation=init_hp['NO_MUTATION'],\n",
    "        architecture=init_hp['ARCHITECTURE_MUTATION'],\n",
    "        new_layer_prob=init_hp['NEW_LAYER_MUTATION'],\n",
    "        parameters=init_hp['PARAMETER_MUTATION'],\n",
    "        activation=init_hp['ACTIVATION_MUTATION'],\n",
    "        rl_hp=init_hp['RL_HP_MUTATION'],\n",
    "        rl_hp_selection=init_hp['RL_HP_SELECTION'],\n",
    "        mutation_sd=init_hp['MUTATION_SD'],\n",
    "        agent_ids=init_hp[\"AGENT_IDS\"],\n",
    "        arch=net_config[\"arch\"],\n",
    "        rand_seed=1,\n",
    "        device=device\n",
    "    )\n",
    "    return mutations\n",
    "\n",
    "\n",
    "def save_trained_model(model, path, filename):\n",
    "    \"\"\"\n",
    "    model: 学習済みのモデル\n",
    "    path: モデルを保存するディレクトリのパス\n",
    "    filename: 保存するファイルの名前\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(model.state_dict(), os.path.join(path, filename))\n",
    "\n",
    "\n",
    "def training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes, max_steps):\n",
    "    epsilon = 1.0\n",
    "    eps_end = 0.1\n",
    "    eps_decay = 0.995\n",
    "    evo_epochs = 20\n",
    "    evo_loop = 1\n",
    "    elite = None\n",
    "\n",
    "    if pop is not None and len(pop) > 0:\n",
    "        elite = pop[0]\n",
    "\n",
    "    for idx_epi in range(max_episodes):\n",
    "        for agent in pop:\n",
    "            state, info = env.reset()\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if init_hp[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info.get(\"agent_mask\")\n",
    "                env_defined_actions = info.get(\"env_defined_actions\")\n",
    "\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                action = discrete_action if agent.discrete_actions else cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "                if init_hp[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                if (memory.counter % agent.learn_step == 0) and (len(memory) >= agent.batch_size):\n",
    "                    experiences = memory.sample(agent.batch_size)\n",
    "                    agent.learn(experiences)\n",
    "\n",
    "                if init_hp[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    swap_channels=init_hp[\"CHANNELS_LAST\"],\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in pop\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in pop]}'\n",
    "            )\n",
    "\n",
    "            if len(pop) > 0:\n",
    "                elite, pop = tournament.select(pop)\n",
    "                pop = mutations.mutation(pop)\n",
    "\n",
    "    if elite is not None:\n",
    "        save_trained_model(elite, './models/MATD3', \"MATD3_trained_agent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat Training Time : 20231225-1616\n",
      "===== AgileRL Online Multi-Agent Demo =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kikunagarikuto/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/6000\n",
      "Fitnesses: ['-133.19', '-109.72', '-241.02', '-174.57']\n",
      "100 fitness avgs: ['-133.19', '-109.72', '-241.02', '-174.57']\n",
      "Episode 40/6000\n",
      "Fitnesses: ['-15.03', '-169.84', '-174.59', '-235.91']\n",
      "100 fitness avgs: ['-62.38', '-151.52', '-153.89', '-184.55']\n",
      "Episode 60/6000\n",
      "Fitnesses: ['-59.79', '-130.28', '-97.03', '-191.88']\n",
      "100 fitness avgs: ['-61.52', '-144.44', '-134.94', '-105.55']\n",
      "Episode 80/6000\n",
      "Fitnesses: ['-35.24', '-569.83', '-112.31', '-100.40']\n",
      "100 fitness avgs: ['-54.95', '-188.60', '-129.28', '-71.24']\n",
      "Episode 100/6000\n",
      "Fitnesses: ['-17.84', '-142.70', '-119.87', '-53.19']\n",
      "100 fitness avgs: ['-47.53', '-85.53', '-67.93', '-67.63']\n",
      "Episode 120/6000\n",
      "Fitnesses: ['-30.62', '-21.67', '-30.32', '-34.18']\n",
      "100 fitness avgs: ['-44.71', '-59.97', '-61.41', '-62.31']\n",
      "Episode 140/6000\n",
      "Fitnesses: ['-328.80', '-24.52', '-153.50', '-79.99']\n",
      "100 fitness avgs: ['-98.37', '-41.82', '-74.57', '-49.75']\n",
      "Episode 160/6000\n",
      "Fitnesses: ['-0.77', '-17.60', '-27.82', '-13.90']\n",
      "100 fitness avgs: ['-36.69', '-45.73', '-89.55', '-38.33']\n",
      "Episode 180/6000\n",
      "Fitnesses: ['-15.75', '-7.16', '-4.22', '-16.31']\n",
      "100 fitness avgs: ['-34.37', '-80.40', '-33.08', '-35.89']\n",
      "Episode 200/6000\n",
      "Fitnesses: ['-73.28', '-113.51', '-135.29', '-88.18']\n",
      "100 fitness avgs: ['-37.10', '-41.13', '-43.31', '-38.59']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m tournament \u001b[38;5;241m=\u001b[39m tournament_selection(init_hp)\n\u001b[1;32m     22\u001b[0m mutations \u001b[38;5;241m=\u001b[39m mutations_config(init_hp, net_config)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtournament\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m str_dt_now \u001b[38;5;241m=\u001b[39m get_current_datetime()\n\u001b[1;32m     28\u001b[0m save_trained_model(pop[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/MATD3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATD3_trained_agent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_dt_now\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 190\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes, max_steps)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (memory\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m%\u001b[39m agent\u001b[38;5;241m.\u001b[39mlearn_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(memory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m    189\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(agent\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 190\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_hp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHANNELS_LAST\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    193\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    194\u001b[0m         agent_id: np\u001b[38;5;241m.\u001b[39mexpand_dims(ns, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m agent_id, ns \u001b[38;5;129;01min\u001b[39;00m next_state\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    196\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/agilerl/algorithms/matd3.py:622\u001b[0m, in \u001b[0;36mMATD3.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    620\u001b[0m             q_value_next_state_2 \u001b[38;5;241m=\u001b[39m critic_target_2(next_input_combined)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         q_value_next_state_1 \u001b[38;5;241m=\u001b[39m \u001b[43mcritic_target_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_input_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m         q_value_next_state_2 \u001b[38;5;241m=\u001b[39m critic_target_2(next_input_combined)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/agilerl/networks/evolvable_mlp.py:294\u001b[0m, in \u001b[0;36mEvolvableMLP.forward\u001b[0;34m(self, x, q)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    293\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 294\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrainbow:\n\u001b[1;32m    297\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    #device = torch.device(\"mps\")\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    str_dt_now = get_current_datetime()\n",
    "    print(f\"Stat Training Time : {str_dt_now}\")\n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "\n",
    "    net_config = define_network_config()\n",
    "    init_hp = initialize_hyperparameters()\n",
    "    env = initialize_environment()\n",
    "\n",
    "    # Set the number of agents in the INIT_HP dictionary\n",
    "    init_hp[\"N_AGENTS\"] = env.num_agents  # Assuming env.agents gives the list of agents\n",
    "    init_hp[\"AGENT_IDS\"] = env.agents  # エージェントIDのリストを設定\n",
    "\n",
    "\n",
    "    state_dim, action_dim, init_hp, one_hot= set_action_and_state_dimensions(env, init_hp)\n",
    "    pop = create_initial_population(init_hp[\"ALGO\"], state_dim, action_dim, one_hot, net_config, init_hp, init_hp[\"POPULATION_SIZE\"], device)\n",
    "    memory = configure_replay_buffer(init_hp, env.agents, device=device)\n",
    "    tournament = tournament_selection(init_hp)\n",
    "    mutations = mutations_config(init_hp, net_config)\n",
    "    \n",
    "    training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes=6000, max_steps=25)\n",
    "    \n",
    "    str_dt_now = get_current_datetime()\n",
    "    \n",
    "    save_trained_model(pop[0], \"./models/MATD3\", f\"MATD3_trained_agent_{str_dt_now}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MATD3' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Load the saved algorithm into the MADDPG object\u001b[39;00m\n\u001b[1;32m     72\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/MATD3/MATD3_trained_agent.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmatd3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Define test loop parameters\u001b[39;00m\n\u001b[1;32m     76\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of episodes to test agent on\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/agilerl/algorithms/matd3.py:1104\u001b[0m, in \u001b[0;36mMATD3.loadCheckpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads saved agent properties and network weights from checkpoint.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m:param path: Location to load checkpoint from\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m:type path: string\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path, pickle_module\u001b[38;5;241m=\u001b[39mdill)\n\u001b[0;32m-> 1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_config \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MATD3' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "\n",
    "# Define function to return image\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm into the MADDPG object\n",
    "    path = \"./models/MATD3/MATD3_trained_agent.pt\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 10  # Number of episodes to test agent on\n",
    "    max_steps = 25  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()\n",
    "\n",
    "    # Save the gif to specified path\n",
    "    gif_path = \"./videos/\"\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        os.path.join(\"./videos/\", \"speaker_listener.gif\"), frames, duration=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
