{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_current_datetime():\n",
    "    # 現在の日時を取得\n",
    "    dt_now = datetime.datetime.now()\n",
    "\n",
    "    # 日時を指定された形式の文字列に変換\n",
    "    str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "    return str_dt_now\n",
    "\n",
    "# この関数を呼び出して現在の日時を取得する例\n",
    "# current_datetime = get_current_datetime()\n",
    "# print(current_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment_setup.py\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "\n",
    "def initialize_environment():\n",
    "    # シンプルなスピーカーリスナー環境を並列環境として定義\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment_setup.py\n",
    "import torch\n",
    "\n",
    "def define_network_config():\n",
    "    return {\n",
    "        \"arch\": \"mlp\",  # ネットワークアーキテクチャ\n",
    "        \"h_size\": [32, 32],  # アクターの隠れ層のサイズ\n",
    "    }\n",
    "\n",
    "def define_initial_hyperparameters():\n",
    "    return {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MATD3\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "        # ... (INIT_HP辞書の残り)\n",
    "    }\n",
    "\n",
    "def get_device():\n",
    "    #return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return torch.device(\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents.py\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "\n",
    "def set_action_and_state_dimensions(env, INIT_HP):\n",
    "    \"\"\"\n",
    "    環境から行動次元と状態次元を設定し、初期ハイパーパラメータを更新する。\n",
    "    env: 学習環境\n",
    "    init_hp: 初期ハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # まず、状態次元を設定する\n",
    "        # 状態空間が離散的か連続的かに基づいて状態次元を取得する\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        # 連続的な状態空間の場合\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "\n",
    "    try:\n",
    "        # 次に、行動次元を設定する\n",
    "        # 行動空間が離散的か連続的かに基づいて行動次元を取得する\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        # 連続的な行動空間の場合\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    if False: # デバッグ出力\n",
    "        print(\"state_dim\", state_dim)\n",
    "        print(\"one_hot\", one_hot)\n",
    "        print( 'INIT_HP[\"DISCRETE_ACTIONS\"]', INIT_HP[\"DISCRETE_ACTIONS\"])\n",
    "        print( 'INIT_HP[\"MAX_ACTION\"]', INIT_HP[\"MAX_ACTION\"])\n",
    "        print( 'INIT_HP[\"MIN_ACTION\"]', INIT_HP[\"MIN_ACTION\"])\n",
    "\n",
    "    # 状態次元の調整（CHANNELS_LAST オプションが True の場合）\n",
    "    if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    return state_dim, action_dim, INIT_HP, one_hot\n",
    "\n",
    "# Create a population ready for evolutionary hyper-parameter optimisation\n",
    "# 進化的なハイパーパラメータ最適化のための母集団を作成する\n",
    "def create_population(INIT_HP, state_dim, action_dim, one_hot, NET_CONFIG, device):\n",
    "    return initialPopulation(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "        # ... (その他のパラメータ)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay_buffer.py\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "\n",
    "def setup_replay_buffer(INIT_HP, device):\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    return MultiAgentReplayBuffer(\n",
    "        INIT_HP[\"MEMORY_SIZE\"],\n",
    "        field_names=field_names,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        device=device,\n",
    "        # ... (その他のパラメータ)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hpo.py\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "\n",
    "# Instantiate a tournament selection object (used for HPO)\n",
    "def tournament_selection(INIT_HP):\n",
    "    return TournamentSelection(\n",
    "        tournament_size=2,  # Tournament selection size\n",
    "        elitism=True,  # Elitism in tournament selection\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "        evo_step=1,\n",
    "        # ... (その他のパラメータ)\n",
    "    )# Evaluate using last N fitness scores\n",
    "\n",
    "# Instantiate a mutations object (used for HPO)\n",
    "def mutations_config(INIT_HP, NET_CONFIG, device):\n",
    "    return Mutations(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        no_mutation=0.2,  # Probability of no mutation\n",
    "        architecture=0.2,  # Probability of architecture mutation\n",
    "        new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "        parameters=0.2,  # Probability of parameter mutation\n",
    "        activation=0,  # Probability of activation function mutation\n",
    "        rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "        rl_hp_selection=[\n",
    "            \"lr\",\n",
    "            \"learn_step\",\n",
    "            \"batch_size\",\n",
    "        ],  # RL hyperparams selected for mutation\n",
    "        mutation_sd=0.1,  # Mutation strength\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        arch=NET_CONFIG[\"arch\"],\n",
    "        rand_seed=2,#1,\n",
    "        device=device,\n",
    "        # ... (その他のパラメータ)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "def training_loop(env, pop ,memory, tournament, mutations, INIT_HP, net_config, max_episodes, max_steps):\n",
    "    \n",
    "    # Define training loop parameters\n",
    "    # 学習ループ・パラメータを定義\n",
    "    max_episodes = 10 # 6000 #500  # Total episodes (default: 6000)\n",
    "    max_steps = 100 #25  # Maximum steps to take in each episode\n",
    "    epsilon = 1.0  # Starting epsilon value\n",
    "    eps_end = 0.1  # Final epsilon value\n",
    "    eps_decay = 0.995  # Epsilon decay\n",
    "    evo_epochs = 20  # Evolution frequency\n",
    "    evo_loop = 1  # Number of evaluation episodes\n",
    "    elite = population[0]  # Assign a placeholder \"elite\" agent\n",
    "    \n",
    "    # Training loop\n",
    "    # 学習ループ\n",
    "    for idx_epi in trange(max_episodes):\n",
    "        \n",
    "        for agent in population:  # Loop through population\n",
    "            \n",
    "            state, info = env.reset()  # Reset environment at start of episode\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "                env_defined_actions = (\n",
    "                    info[\"env_defined_actions\"]\n",
    "                    if \"env_defined_actions\" in info.keys()\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "                # Get next action from agent\n",
    "                # エージェントから次の行動を取得する\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                if agent.discrete_actions:\n",
    "                    action = discrete_action\n",
    "                else:\n",
    "                    action = cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(\n",
    "                    action\n",
    "                )  # Act in environment\n",
    "\n",
    "                # Image processing if necessary for the environment\n",
    "                # 環境に応じた画像処理を行う\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                # Save experiences to replay buffer\n",
    "                # 経験をリプレイバッファに保存する\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                # Collect the reward\n",
    "                # 報酬を受け取る\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                # Learn according to learning frequency\n",
    "                # 学習周期に合わせて学習する\n",
    "                if (memory.counter % agent.learn_step == 0) and (\n",
    "                    len(memory) >= agent.batch_size\n",
    "                ):\n",
    "                    experiences = memory.sample(\n",
    "                        agent.batch_size\n",
    "                    )  # Sample replay buffer\n",
    "                    agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "                # Update the state\n",
    "                # 状態を更新する\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                # Stop episode if any agents have terminated\n",
    "                # いずれかのエージェントが終了したならば、エピソードを停止する\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            # Save the total episode reward\n",
    "            # エピソードの合計報酬を保存する\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        # Update epsilon for exploration\n",
    "        # 探索用のイプシロンを更新\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        # Now evolve population if necessary\n",
    "        # 必要であれば、母集団を進化させる\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            # Evaluate population\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in population\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in population]}'\n",
    "            )\n",
    "\n",
    "            # Tournament selection and population mutation\n",
    "            # トーナメント選択と母集団の変異\n",
    "            elite, population = tournament.select(population)\n",
    "            population = mutations.mutation(population)\n",
    "\n",
    "    # Save the trained algorithm\n",
    "    # 学習アルゴリズムを保存する\n",
    "    #path = \"./models/MATD3\"\n",
    "    path = \"./\"+str_dt_now+\"/models/MATD3\"\n",
    "    filename = \"MATD3_trained_agent.pt\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_path = os.path.join(path, filename)\n",
    "    elite.saveCheckpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== AgileRL Online Multi-Agent Demo =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'define_network_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# ... (トレーニングループの残りのコード)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== AgileRL Online Multi-Agent Demo =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m NET_CONFIG \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_network_config\u001b[49m()\n\u001b[1;32m     23\u001b[0m INIT_HP \u001b[38;5;241m=\u001b[39m define_initial_hyperparameters()\n\u001b[1;32m     24\u001b[0m env \u001b[38;5;241m=\u001b[39m initialize_environment()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'define_network_config' is not defined"
     ]
    }
   ],
   "source": [
    "#main.py\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "\n",
    "# モジュールをインポート\n",
    "#import environment_setup\n",
    "#import config\n",
    "#import agents\n",
    "#import replay_buffer\n",
    "#import hpo\n",
    "#import train\n",
    "\n",
    "def main():\n",
    "    device = get_device()\n",
    "    \n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "    \n",
    "    NET_CONFIG = define_network_config()\n",
    "    INIT_HP = define_initial_hyperparameters()\n",
    "    env = initialize_environment()\n",
    "    \n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    # エージェントとエージェントIDの数を、ハイパーパラメータディクショナリの初期化のために加える\n",
    "    INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "    INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "    \n",
    "    if True: # デバッグ出力\n",
    "        print('state_dim', state_dim)\n",
    "        print('action_dim', action_dim)\n",
    "        print('one_hot', one_hot)\n",
    "        print('NET_CONFIG')\n",
    "        pprint.pprint(NET_CONFIG)\n",
    "        print('INIT_HP')\n",
    "        pprint.pprint(INIT_HP)\n",
    "        print('device', device)\n",
    "    \n",
    "    state_dim, action_dim, INIT_HP, one_hot = set_action_and_state_dimensions(env, INIT_HP)\n",
    "    pop = create_population(INIT_HP[\"ALGO\"], state_dim, action_dim, one_hot, NET_CONFIG, INIT_HP, INIT_HP[\"POPULATION_SIZE\"], device)\n",
    "    memory = setup_replay_buffer(INIT_HP, env.agent, device=device)\n",
    "    tournament = tournament_selection(INIT_HP)\n",
    "    mutations = mutations_config(INIT_HP, NET_CONFIG)\n",
    "    \n",
    "    training_loop(env, pop ,memory, tournament, mutations, INIT_HP, NET_CONFIG, max_episodes=6000, max_steps=25)\n",
    "    \n",
    "    \n",
    "    # ... (トレーニングループの残りのコード)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment.py\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "\n",
    "def setup_environment(render_mode=\"rgb_array\"):\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True, render_mode=render_mode)\n",
    "    env.reset()\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "    text_color = (255, 255, 255) if np.mean(frame) < 128 else (0, 0, 0)\n",
    "    drawer.text((im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color)\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.py\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "def load_matd3_agent(path, state_dim, action_dim, one_hot, n_agents, agent_ids, max_action, min_action, discrete_actions, device):\n",
    "    matd3 = MATD3(state_dim, action_dim, one_hot, n_agents, agent_ids, max_action, min_action, discrete_actions, device=device)\n",
    "    matd3.loadCheckpoint(path)\n",
    "    return matd3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# モジュールのインポート\n",
    "import environment\n",
    "import utils\n",
    "import agent\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = environment.setup_environment()\n",
    "    # ... (残りのメインスクリプトコード)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
