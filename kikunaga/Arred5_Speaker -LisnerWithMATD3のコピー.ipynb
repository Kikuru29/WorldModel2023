{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pettingzoo[mpe]\n",
    "#!pip install agilerl\n",
    "#!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "\n",
    "\n",
    "###\n",
    "#import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "\n",
    "def get_current_datetime():\n",
    "    # 現在の日時を取得\n",
    "    dt_now = datetime.datetime.now()\n",
    "\n",
    "    # 日時を指定された形式の文字列に変換\n",
    "    str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "    return str_dt_now\n",
    "\n",
    "# この関数を呼び出して現在の日時を取得する例\n",
    "# current_datetime = get_current_datetime()\n",
    "# print(current_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network configuration\n",
    "def define_network_config():\n",
    "    return {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Actor hidden size\n",
    "    }\n",
    "\n",
    "# Define the initial hyperparameters\n",
    "def initialize_hyperparameters():\n",
    "    return {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MATD3\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "        # Instantiate a tournament selection object (used for HPO)\n",
    "        'TOURNAMENT_SIZE': 2,\n",
    "        'ELITISM': True,\n",
    "        # Instantiate a mutations object (used for HPO)\n",
    "        'NO_MUTATION': 0.2,\n",
    "        'ARCHITECTURE_MUTATION': 0.2,\n",
    "        'NEW_LAYER_MUTATION': 0.2,\n",
    "        'PARAMETER_MUTATION': 0.2,\n",
    "        'ACTIVATION_MUTATION': 0,\n",
    "        'RL_HP_MUTATION': 0.2,\n",
    "        'RL_HP_SELECTION': [\"lr\", \"learn_step\", \"batch_size\"], # RL hyperparams selected for mutation\n",
    "        'MUTATION_SD': 0.1,\n",
    "\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple speaker listener environment as a parallel environment\n",
    "def initialize_environment():\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "# Configure the multi-agent algo input arguments\n",
    "def set_action_and_state_dimensions(env, init_hp):\n",
    "    \"\"\"\n",
    "    環境から行動次元と状態次元を設定し、初期ハイパーパラメータを更新する。\n",
    "    env: 学習環境\n",
    "    init_hp: 初期ハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # まず、状態次元を設定する\n",
    "        # 状態空間が離散的か連続的かに基づいて状態次元を取得する\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        # 連続的な状態空間の場合\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "\n",
    "    try:\n",
    "        # 次に、行動次元を設定する\n",
    "        # 行動空間が離散的か連続的かに基づいて行動次元を取得する\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        init_hp[\"DISCRETE_ACTIONS\"] = True\n",
    "        init_hp[\"MAX_ACTION\"] = None\n",
    "        init_hp[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        # 連続的な行動空間の場合\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        init_hp[\"DISCRETE_ACTIONS\"] = False\n",
    "        init_hp[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        init_hp[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # 状態次元の調整（CHANNELS_LAST オプションが True の場合）\n",
    "    if init_hp[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    return state_dim, action_dim, init_hp, one_hot\n",
    "\n",
    "\n",
    "def create_initial_population(algo, state_dim, action_dim, one_hot, net_config, init_hp, population_size, device):\n",
    "    \"\"\"\n",
    "    初期人口を生成する。\n",
    "    algo: 使用する強化学習アルゴリズム\n",
    "    state_dim: 状態次元\n",
    "    action_dim: 行動次元\n",
    "    one_hot: 状態がワンホットエンコードされているかどうか\n",
    "    net_config: ネットワーク構成\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    device: 使用するデバイス（例: \"cuda\"、\"mps\"、\"cpu\"）\n",
    "    \"\"\"\n",
    "    pop = initialPopulation(\n",
    "        init_hp[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        net_config,\n",
    "        init_hp,\n",
    "        population_size=init_hp[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "    if pop is None:\n",
    "        return []\n",
    "    else:\n",
    "        return pop\n",
    "\n",
    "\n",
    "def configure_replay_buffer(init_hp, field_names, device):\n",
    "    \"\"\"\n",
    "    リプレイバッファを設定する。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    agent_ids: エージェントのIDリスト\n",
    "    device: 使用するデバイス（例: \"cuda\"、\"mps\"、\"cpu\"）\n",
    "    \"\"\"\n",
    "    # リプレイバッファを格納するためのデータ構造を定義\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "\n",
    "    # リプレイバッファのインスタンスを作成\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        init_hp[\"MEMORY_SIZE\"],  # バッファの最大サイズ\n",
    "        field_names=field_names,  # 格納するフィールド名\n",
    "        agent_ids=init_hp[\"AGENT_IDS\"],      # エージェントのID\n",
    "        device=device,             # 使用するデバイス\n",
    "    )\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "def tournament_selection(init_hp):\n",
    "    \"\"\"\n",
    "    トーナメント選択の設定を行う。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    \"\"\"\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=init_hp['TOURNAMENT_SIZE'],\n",
    "        elitism=init_hp['ELITISM'],\n",
    "        population_size=init_hp['POPULATION_SIZE'],\n",
    "        evo_step =1,\n",
    "    )\n",
    "    return tournament\n",
    "\n",
    "\n",
    "def mutations_config(init_hp, net_config):\n",
    "    \"\"\"\n",
    "    突然変異の設定を行う。\n",
    "    init_hp: 初期ハイパーパラメータ\n",
    "    net_config: ネットワーク構成\n",
    "    \"\"\"\n",
    "    mutations = Mutations(\n",
    "        algo=init_hp[\"ALGO\"],\n",
    "        no_mutation=init_hp['NO_MUTATION'],\n",
    "        architecture=init_hp['ARCHITECTURE_MUTATION'],\n",
    "        new_layer_prob=init_hp['NEW_LAYER_MUTATION'],\n",
    "        parameters=init_hp['PARAMETER_MUTATION'],\n",
    "        activation=init_hp['ACTIVATION_MUTATION'],\n",
    "        rl_hp=init_hp['RL_HP_MUTATION'],\n",
    "        rl_hp_selection=init_hp['RL_HP_SELECTION'],\n",
    "        mutation_sd=init_hp['MUTATION_SD'],\n",
    "        agent_ids=init_hp[\"AGENT_IDS\"],\n",
    "        arch=net_config[\"arch\"],\n",
    "        rand_seed=1,\n",
    "        device=device\n",
    "    )\n",
    "    return mutations\n",
    "\n",
    "\n",
    "def save_trained_model(model, path, filename):\n",
    "    \"\"\"\n",
    "    model: 学習済みのモデル\n",
    "    path: モデルを保存するディレクトリのパス\n",
    "    filename: 保存するファイルの名前\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(model, os.path.join(path, filename))\n",
    "\n",
    "\n",
    "def training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes, max_steps):\n",
    "    epsilon = 1.0\n",
    "    eps_end = 0.1\n",
    "    eps_decay = 0.995\n",
    "    evo_epochs = 20\n",
    "    evo_loop = 1\n",
    "    elite = None\n",
    "\n",
    "    if pop is not None and len(pop) > 0:\n",
    "        elite = pop[0]\n",
    "\n",
    "    for idx_epi in range(max_episodes):\n",
    "        for agent in pop:\n",
    "            state, info = env.reset()\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if init_hp[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info.get(\"agent_mask\")\n",
    "                env_defined_actions = info.get(\"env_defined_actions\")\n",
    "\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                action = discrete_action if agent.discrete_actions else cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "                if init_hp[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                if (memory.counter % agent.learn_step == 0) and (len(memory) >= agent.batch_size):\n",
    "                    experiences = memory.sample(agent.batch_size)\n",
    "                    agent.learn(experiences)\n",
    "\n",
    "                if init_hp[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    swap_channels=init_hp[\"CHANNELS_LAST\"],\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in pop\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in pop]}'\n",
    "            )\n",
    "\n",
    "            if len(pop) > 0:\n",
    "                elite, pop = tournament.select(pop)\n",
    "                pop = mutations.mutation(pop)\n",
    "\n",
    "    if elite is not None:\n",
    "        save_trained_model(elite, './models/MATD3', \"MATD3_trained_agent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat Training Time : 20231225-1739\n",
      "===== AgileRL Online Multi-Agent Demo =====\n",
      "Episode 20/240\n",
      "Fitnesses: ['-8.98', '-375.32', '-141.16', '-216.14']\n",
      "100 fitness avgs: ['-8.98', '-375.32', '-141.16', '-216.14']\n",
      "Episode 40/240\n",
      "Fitnesses: ['-127.86', '-32.66', '-147.92', '-51.29']\n",
      "100 fitness avgs: ['-68.42', '-124.40', '-182.03', '-30.13']\n",
      "Episode 60/240\n",
      "Fitnesses: ['-231.93', '-34.44', '-135.91', '-104.74']\n",
      "100 fitness avgs: ['-160.24', '-94.42', '-65.39', '-117.85']\n",
      "Episode 80/240\n",
      "Fitnesses: ['-19.49', '-99.88', '-282.21', '-65.86']\n",
      "100 fitness avgs: ['-75.68', '-113.36', '-141.36', '-104.85']\n",
      "Episode 100/240\n",
      "Fitnesses: ['-27.74', '-178.38', '-119.99', '-90.79']\n",
      "100 fitness avgs: ['-66.09', '-119.56', '-84.54', '-78.70']\n",
      "Episode 120/240\n",
      "Fitnesses: ['-69.64', '-41.99', '-86.40', '-315.89']\n",
      "100 fitness avgs: ['-66.68', '-72.58', '-69.48', '-107.73']\n",
      "Episode 140/240\n",
      "Fitnesses: ['-69.11', '-111.06', '-25.85', '-35.72']\n",
      "100 fitness avgs: ['-72.09', '-78.08', '-63.25', '-67.32']\n",
      "Episode 160/240\n",
      "Fitnesses: ['-16.87', '-28.97', '-3.88', '-23.72']\n",
      "100 fitness avgs: ['-57.45', '-62.53', '-63.56', '-58.30']\n",
      "Episode 180/240\n",
      "Fitnesses: ['-21.07', '-59.71', '-317.97', '-48.08']\n",
      "100 fitness avgs: ['-58.84', '-62.21', '-87.16', '-56.41']\n",
      "Episode 200/240\n",
      "Fitnesses: ['-16.07', '-17.01', '-36.33', '-24.96']\n",
      "100 fitness avgs: ['-54.56', '-54.66', '-59.62', '-55.45']\n",
      "Episode 220/240\n",
      "Fitnesses: ['-46.73', '-7.46', '-19.40', '-7.93']\n",
      "100 fitness avgs: ['-53.85', '-50.28', '-51.37', '-54.92']\n",
      "Episode 240/240\n",
      "Fitnesses: ['-16.70', '-59.15', '-49.75', '-11.86']\n",
      "100 fitness avgs: ['-47.48', '-52.02', '-51.23', '-47.08']\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    #device = torch.device(\"mps\")\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    str_dt_now = get_current_datetime()\n",
    "    print(f\"Stat Training Time : {str_dt_now}\")\n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "\n",
    "    net_config = define_network_config()\n",
    "    init_hp = initialize_hyperparameters()\n",
    "    env = initialize_environment()\n",
    "\n",
    "    # Set the number of agents in the INIT_HP dictionary\n",
    "    init_hp[\"N_AGENTS\"] = env.num_agents  # Assuming env.agents gives the list of agents\n",
    "    init_hp[\"AGENT_IDS\"] = env.agents  # エージェントIDのリストを設定\n",
    "\n",
    "\n",
    "    state_dim, action_dim, init_hp, one_hot= set_action_and_state_dimensions(env, init_hp)\n",
    "    pop = create_initial_population(init_hp[\"ALGO\"], state_dim, action_dim, one_hot, net_config, init_hp, init_hp[\"POPULATION_SIZE\"], device)\n",
    "    memory = configure_replay_buffer(init_hp, env.agents, device=device)\n",
    "    tournament = tournament_selection(init_hp)\n",
    "    mutations = mutations_config(init_hp, net_config)\n",
    "    \n",
    "    training_loop(env, pop, memory, tournament, mutations, init_hp, net_config, max_episodes=240, max_steps=25)\n",
    "    \n",
    "    str_dt_now = get_current_datetime()\n",
    "\n",
    "    filename = f\"MATD3_trained_agent_{str_dt_now}.pt\"\n",
    "    \n",
    "    save_trained_model(pop[0], \"./models/MATD3\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MATD3' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Load the saved algorithm into the MADDPG object\u001b[39;00m\n\u001b[1;32m     72\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/MATD3/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmatd3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Define test loop parameters\u001b[39;00m\n\u001b[1;32m     76\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of episodes to test agent on\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/agilerl/algorithms/matd3.py:1104\u001b[0m, in \u001b[0;36mMATD3.loadCheckpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads saved agent properties and network weights from checkpoint.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m:param path: Location to load checkpoint from\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m:type path: string\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path, pickle_module\u001b[38;5;241m=\u001b[39mdill)\n\u001b[0;32m-> 1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_config \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MATD3' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "\n",
    "# Define function to return image\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm into the MADDPG object\n",
    "    path = f\"./models/MATD3/{filename}\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 10  # Number of episodes to test agent on\n",
    "    max_steps = 25  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()\n",
    "\n",
    "    # Save the gif to specified path\n",
    "    gif_path = \"./videos/\"\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        os.path.join(\"./videos/\", f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m env \u001b[38;5;241m=\u001b[39m initialize_environment()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# モデルの評価\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 結果のプロット\u001b[39;00m\n\u001b[1;32m     53\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, env, episodes)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# getAction メソッドが辞書形式の状態を期待しているため、適切な形式に変換\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgetAction({agent: state[agent] \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents}, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     31\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(reward\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     32\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    保存されたモデルをロードする。\n",
    "    path: モデルファイルのパス\n",
    "    \"\"\"\n",
    "    path = f\"./models/MATD3/{filename}\"\n",
    "    model = torch.load(path)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, env, episodes=100):\n",
    "    \"\"\"\n",
    "    モデルを評価する。\n",
    "    model: 評価するモデル\n",
    "    env: 環境\n",
    "    episodes: 評価に使用するエピソード数\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()  # 状態と追加情報を返す\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # getAction メソッドが辞書形式の状態を期待しているため、適切な形式に変換\n",
    "            action = model.getAction({agent: state[agent] for agent in env.agents}, epsilon=0)[0]\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += sum(reward.values())\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# モデルのロード\n",
    "path = \"./models/MATD3/MATD3_trained_agent.pt\"\n",
    "model = load_model(path)\n",
    "\n",
    "# 環境の初期化\n",
    "env = initialize_environment()\n",
    "\n",
    "# モデルの評価\n",
    "rewards = evaluate_model(model, env)\n",
    "\n",
    "# 結果のプロット\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Model Performance')\n",
    "plt.show()\n",
    "\n",
    "# パフォーマンスの統計\n",
    "print(\"Average Reward:\", np.mean(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
