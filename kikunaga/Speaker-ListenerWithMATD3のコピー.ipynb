{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4488f4",
   "metadata": {},
   "source": [
    "# AgileRL Speaker-Listener with MATD3\n",
    "https://docs.agilerl.com/en/latest/tutorials/pettingzoo/matd3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b039f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfadb438-9362-4b9d-9d05-ae54b857ffcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python3 -m pip install playsound\n",
    "#!python3 -m pip install PyObjC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2e6aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pettingzoo[mpe]\n",
    "#!pip install agilerl\n",
    "#!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c21e64-918d-4fe1-82a0-41edff4c17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231226-0652\n"
     ]
    }
   ],
   "source": [
    "#現在日時を取得\n",
    "import datetime\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "print(str_dt_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9d627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231226-0652\n",
      "===== AgileRL Online Multi-Agent Demo =====\n",
      "state_dim [(3,), (11,)]\n",
      "action_dim [3, 5]\n",
      "one_hot False\n",
      "NET_CONFIG\n",
      "{'arch': 'mlp', 'h_size': [32, 32]}\n",
      "INIT_HP\n",
      "{'AGENT_IDS': ['speaker_0', 'listener_0'],\n",
      " 'ALGO': 'MATD3',\n",
      " 'BATCH_SIZE': 32,\n",
      " 'CHANNELS_LAST': False,\n",
      " 'DISCRETE_ACTIONS': False,\n",
      " 'GAMMA': 0.95,\n",
      " 'LEARN_STEP': 5,\n",
      " 'LR': 0.01,\n",
      " 'MAX_ACTION': [array([1., 1., 1.], dtype=float32),\n",
      "                array([1., 1., 1., 1., 1.], dtype=float32)],\n",
      " 'MEMORY_SIZE': 100000,\n",
      " 'MIN_ACTION': [array([0., 0., 0.], dtype=float32),\n",
      "                array([0., 0., 0., 0., 0.], dtype=float32)],\n",
      " 'N_AGENTS': 2,\n",
      " 'POLICY_FREQ': 2,\n",
      " 'POPULATION_SIZE': 4,\n",
      " 'TAU': 0.01}\n",
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                       | 19/6000 [00:10<54:25,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/6000\n",
      "Fitnesses: ['-12.26', '-483.91', '-371.99', '-252.56']\n",
      "100 fitness avgs: ['-12.26', '-483.91', '-371.99', '-252.56']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                                                       | 39/6000 [00:21<55:46,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40/6000\n",
      "Fitnesses: ['-485.64', '-191.07', '-127.47', '-408.97']\n",
      "100 fitness avgs: ['-248.95', '-221.82', '-249.73', '-210.62']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                      | 59/6000 [00:33<57:13,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60/6000\n",
      "Fitnesses: ['-86.86', '-107.03', '-62.36', '-76.29']\n",
      "100 fitness avgs: ['-195.44', '-176.09', '-161.20', '-165.84']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                    | 79/6000 [00:45<1:01:32,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80/6000\n",
      "Fitnesses: ['-2.06', '-44.94', '-6.49', '-1.50']\n",
      "100 fitness avgs: ['-121.41', '-135.62', '-126.00', '-121.27']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                                                    | 99/6000 [00:58<1:02:36,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/6000\n",
      "Fitnesses: ['-0.24', '-31.62', '-46.62', '-44.57']\n",
      "100 fitness avgs: ['-97.06', '-103.34', '-106.45', '-117.41']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▎                                                                                                                  | 119/6000 [01:12<1:09:52,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 120/6000\n",
      "Fitnesses: ['-85.59', '-194.45', '-297.91', '-212.42']\n",
      "100 fitness avgs: ['-95.15', '-113.29', '-130.54', '-124.11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▋                                                                                                                  | 139/6000 [01:27<1:11:39,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 140/6000\n",
      "Fitnesses: ['-41.68', '-13.87', '-184.13', '-286.03']\n",
      "100 fitness avgs: ['-87.51', '-99.09', '-107.86', '-152.75']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███                                                                                                                  | 159/6000 [01:42<1:10:02,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 160/6000\n",
      "Fitnesses: ['-2.02', '-314.99', '-600.70', '-207.60']\n",
      "100 fitness avgs: ['-86.96', '-115.95', '-169.47', '-112.66']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▌                                                                                                                   | 179/6000 [01:54<57:12,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180/6000\n",
      "Fitnesses: ['-62.80', '-115.66', '-31.85', '-2.87']\n",
      "100 fitness avgs: ['-84.27', '-90.15', '-80.83', '-77.61']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▉                                                                                                                 | 199/6000 [02:07<1:00:16,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/6000\n",
      "Fitnesses: ['-13.88', '-46.35', '-4.56', '-27.00']\n",
      "100 fitness avgs: ['-71.24', '-80.48', '-73.21', '-75.45']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▎                                                                                                                | 219/6000 [02:22<1:10:55,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 220/6000\n",
      "Fitnesses: ['-107.45', '-5.80', '-74.92', '-46.98']\n",
      "100 fitness avgs: ['-76.32', '-65.29', '-71.58', '-69.04']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▋                                                                                                                | 239/6000 [02:38<1:15:42,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 240/6000\n",
      "Fitnesses: ['-29.59', '-45.41', '-51.60', '-17.65']\n",
      "100 fitness avgs: ['-62.32', '-63.64', '-64.15', '-67.08']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████                                                                                                                | 259/6000 [02:54<1:16:13,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 260/6000\n",
      "Fitnesses: ['-15.55', '-29.56', '-4.11', '-85.11']\n",
      "100 fitness avgs: ['-63.12', '-64.20', '-57.84', '-64.07']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▍                                                                                                               | 279/6000 [03:10<1:16:06,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 280/6000\n",
      "Fitnesses: ['-45.62', '-92.23', '-16.05', '-38.45']\n",
      "100 fitness avgs: ['-56.97', '-60.30', '-59.76', '-56.45']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▊                                                                                                               | 299/6000 [03:27<1:17:13,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300/6000\n",
      "Fitnesses: ['-10.72', '-23.73', '-32.86', '-17.03']\n",
      "100 fitness avgs: ['-56.49', '-57.35', '-54.88', '-56.91']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▏                                                                                                              | 319/6000 [03:43<1:17:19,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 320/6000\n",
      "Fitnesses: ['-23.80', '-86.51', '-23.22', '-63.66']\n",
      "100 fitness avgs: ['-54.44', '-58.36', '-54.41', '-56.93']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████▌                                                                                                              | 339/6000 [04:00<1:17:23,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 340/6000\n",
      "Fitnesses: ['-4.34', '-20.30', '-66.24', '-5.07']\n",
      "100 fitness avgs: ['-51.46', '-54.78', '-55.10', '-51.50']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████                                                                                                              | 359/6000 [04:17<1:17:26,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 360/6000\n",
      "Fitnesses: ['-31.05', '-95.75', '-73.58', '-25.95']\n",
      "100 fitness avgs: ['-50.33', '-53.92', '-52.73', '-50.04']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▍                                                                                                             | 379/6000 [04:33<1:17:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 380/6000\n",
      "Fitnesses: ['-16.10', '-11.57', '-114.94', '-57.25']\n",
      "100 fitness avgs: ['-48.26', '-50.56', '-53.46', '-50.42']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▊                                                                                                             | 399/6000 [04:50<1:17:29,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400/6000\n",
      "Fitnesses: ['-41.73', '-19.92', '-17.93', '-7.20']\n",
      "100 fitness avgs: ['-50.12', '-46.84', '-48.93', '-48.26']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████▏                                                                                                            | 419/6000 [05:09<1:26:30,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 420/6000\n",
      "Fitnesses: ['-14.53', '-102.39', '-16.04', '-8.02']\n",
      "100 fitness avgs: ['-46.66', '-51.48', '-46.73', '-44.99']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████▌                                                                                                            | 439/6000 [05:28<1:25:36,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 440/6000\n",
      "Fitnesses: ['-261.26', '-46.16', '-140.81', '-72.44']\n",
      "100 fitness avgs: ['-54.82', '-51.24', '-50.94', '-47.83']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████▉                                                                                                            | 459/6000 [05:47<1:25:39,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 460/6000\n",
      "Fitnesses: ['-23.52', '-165.19', '-8.39', '-100.69']\n",
      "100 fitness avgs: ['-50.03', '-52.93', '-49.37', '-53.39']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████████▎                                                                                                           | 479/6000 [06:03<1:12:16,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 480/6000\n",
      "Fitnesses: ['-72.39', '-96.82', '-6.32', '-17.76']\n",
      "100 fitness avgs: ['-50.33', '-55.20', '-47.58', '-48.06']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████████▋                                                                                                           | 499/6000 [06:20<1:16:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/6000\n",
      "Fitnesses: ['-21.92', '-9.42', '-5.32', '-34.45']\n",
      "100 fitness avgs: ['-46.55', '-46.51', '-46.35', '-47.05']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████████                                                                                                           | 519/6000 [06:37<1:15:07,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 520/6000\n",
      "Fitnesses: ['-32.70', '-3.66', '-27.59', '-2.17']\n",
      "100 fitness avgs: ['-45.82', '-44.90', '-45.82', '-44.81']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████████▌                                                                                                          | 539/6000 [06:54<1:14:26,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 540/6000\n",
      "Fitnesses: ['-51.93', '-32.53', '-15.68', '-62.26']\n",
      "100 fitness avgs: ['-45.07', '-44.35', '-43.73', '-46.43']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████████▉                                                                                                          | 559/6000 [07:10<1:13:54,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 560/6000\n",
      "Fitnesses: ['-40.78', '-19.59', '-31.86', '-3.47']\n",
      "100 fitness avgs: ['-43.62', '-43.47', '-43.90', '-42.89']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▎                                                                                                         | 579/6000 [07:27<1:13:27,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 580/6000\n",
      "Fitnesses: ['-38.79', '-21.13', '-23.41', '-12.60']\n",
      "100 fitness avgs: ['-42.75', '-42.70', '-42.77', '-42.82']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▋                                                                                                         | 599/6000 [07:43<1:12:51,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600/6000\n",
      "Fitnesses: ['-43.57', '-45.76', '-28.20', '-40.30']\n",
      "100 fitness avgs: ['-42.85', '-42.87', '-42.34', '-42.74']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████                                                                                                         | 619/6000 [08:02<1:22:10,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 620/6000\n",
      "Fitnesses: ['-32.07', '-54.13', '-15.43', '-26.73']\n",
      "100 fitness avgs: ['-42.01', '-43.11', '-41.47', '-42.22']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████▍                                                                                                        | 639/6000 [08:19<1:15:46,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 640/6000\n",
      "Fitnesses: ['-13.88', '-73.84', '-12.11', '-24.51']\n",
      "100 fitness avgs: ['-40.61', '-43.00', '-40.55', '-41.67']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████▊                                                                                                        | 659/6000 [08:36<1:11:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 660/6000\n",
      "Fitnesses: ['-44.22', '-3.49', '-28.72', '-18.12']\n",
      "100 fitness avgs: ['-40.66', '-39.43', '-40.19', '-39.87']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████████▏                                                                                                       | 679/6000 [08:52<1:10:09,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 680/6000\n",
      "Fitnesses: ['-54.30', '-42.80', '-54.97', '-24.79']\n",
      "100 fitness avgs: ['-39.87', '-39.53', '-40.63', '-39.00']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████▋                                                                                                       | 699/6000 [09:09<1:10:42,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700/6000\n",
      "Fitnesses: ['-13.95', '-16.39', '-26.70', '-68.57']\n",
      "100 fitness avgs: ['-38.28', '-38.35', '-39.16', '-39.84']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████                                                                                                       | 719/6000 [09:25<1:10:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 720/6000\n",
      "Fitnesses: ['-29.15', '-30.47', '-36.35', '-16.67']\n",
      "100 fitness avgs: ['-38.03', '-38.13', '-38.23', '-37.75']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████▍                                                                                                      | 739/6000 [09:42<1:09:23,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 740/6000\n",
      "Fitnesses: ['-32.96', '-1.81', '-64.20', '-27.63']\n",
      "100 fitness avgs: ['-37.62', '-37.05', '-38.46', '-37.48']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████▊                                                                                                      | 759/6000 [09:58<1:05:43,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 760/6000\n",
      "Fitnesses: ['-4.78', '-10.55', '-61.30', '-18.46']\n",
      "100 fitness avgs: ['-36.20', '-36.77', '-37.69', '-36.98']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████▏                                                                                                     | 779/6000 [10:16<1:18:48,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 780/6000\n",
      "Fitnesses: ['-15.65', '-43.57', '-37.62', '-13.82']\n",
      "100 fitness avgs: ['-35.67', '-37.14', '-36.99', '-35.63']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████▌                                                                                                     | 799/6000 [10:34<1:17:15,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800/6000\n",
      "Fitnesses: ['-72.88', '-2.35', '-18.86', '-28.71']\n",
      "100 fitness avgs: ['-36.56', '-36.13', '-35.21', '-35.50']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████████▉                                                                                                     | 819/6000 [10:52<1:14:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 820/6000\n",
      "Fitnesses: ['-22.47', '-135.10', '-149.12', '-14.75']\n",
      "100 fitness avgs: ['-35.79', '-38.54', '-38.88', '-34.99']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████▎                                                                                                    | 839/6000 [11:13<1:31:21,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 840/6000\n",
      "Fitnesses: ['-35.77', '-15.87', '-0.80', '-93.99']\n",
      "100 fitness avgs: ['-35.01', '-34.54', '-34.18', '-39.86']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████▊                                                                                                    | 859/6000 [11:35<1:31:06,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 860/6000\n",
      "Fitnesses: ['-62.00', '-4.62', '-70.29', '-91.03']\n",
      "100 fitness avgs: ['-34.83', '-33.49', '-35.83', '-41.05']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▏                                                                                                   | 879/6000 [11:59<1:42:24,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 880/6000\n",
      "Fitnesses: ['-81.84', '-6.72', '-31.54', '-27.38']\n",
      "100 fitness avgs: ['-34.59', '-32.88', '-33.45', '-34.66']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▌                                                                                                   | 899/6000 [12:23<1:42:17,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900/6000\n",
      "Fitnesses: ['-17.04', '-91.52', '-62.12', '-83.50']\n",
      "100 fitness avgs: ['-32.53', '-34.19', '-33.53', '-34.56']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▉                                                                                                   | 919/6000 [12:47<1:40:35,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 920/6000\n",
      "Fitnesses: ['-21.76', '-35.11', '-53.68', '-58.82']\n",
      "100 fitness avgs: ['-32.30', '-32.59', '-34.61', '-33.10']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████▎                                                                                                  | 939/6000 [13:13<1:52:25,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 940/6000\n",
      "Fitnesses: ['-16.24', '-15.26', '-68.94', '-39.49']\n",
      "100 fitness avgs: ['-31.96', '-32.22', '-33.36', '-32.73']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████▋                                                                                                  | 959/6000 [13:43<2:09:07,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 960/6000\n",
      "Fitnesses: ['-14.77', '-23.58', '-5.83', '-17.80']\n",
      "100 fitness avgs: ['-31.86', '-32.54', '-31.67', '-31.92']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████                                                                                                  | 979/6000 [14:17<2:18:41,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 980/6000\n",
      "Fitnesses: ['-57.73', '-1.67', '-10.01', '-4.49']\n",
      "100 fitness avgs: ['-32.20', '-31.30', '-31.41', '-31.30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████████▍                                                                                                 | 999/6000 [14:50<2:19:04,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/6000\n",
      "Fitnesses: ['-87.41', '-19.53', '-6.71', '-24.07']\n",
      "100 fitness avgs: ['-32.42', '-31.06', '-30.81', '-31.16']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████████▋                                                                                                | 1019/6000 [15:24<2:22:17,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1020/6000\n",
      "Fitnesses: ['-12.62', '-16.13', '-12.06', '-37.64']\n",
      "100 fitness avgs: ['-30.45', '-30.52', '-30.78', '-31.19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████████                                                                                                | 1039/6000 [15:59<2:23:41,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1040/6000\n",
      "Fitnesses: ['-12.29', '-98.34', '-7.99', '-27.54']\n",
      "100 fitness avgs: ['-30.43', '-32.08', '-30.34', '-30.72']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████████████▍                                                                                               | 1059/6000 [16:33<2:22:48,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1060/6000\n",
      "Fitnesses: ['-15.34', '-68.81', '-12.55', '-28.06']\n",
      "100 fitness avgs: ['-30.06', '-31.07', '-30.01', '-30.67']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████████████▊                                                                                               | 1079/6000 [17:05<2:15:10,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1080/6000\n",
      "Fitnesses: ['-6.95', '-9.54', '-54.70', '-7.60']\n",
      "100 fitness avgs: ['-29.58', '-29.63', '-30.52', '-29.59']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████▏                                                                                              | 1099/6000 [17:37<2:13:23,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100/6000\n",
      "Fitnesses: ['-16.79', '-14.18', '-25.06', '-29.77']\n",
      "100 fitness avgs: ['-29.35', '-29.30', '-30.42', '-29.60']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████████████▋                                                                                              | 1119/6000 [18:10<2:13:31,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1120/6000\n",
      "Fitnesses: ['-24.88', '-76.30', '-87.65', '-43.93']\n",
      "100 fitness avgs: ['-29.22', '-30.14', '-31.44', '-30.66']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████████████                                                                                              | 1139/6000 [18:41<2:03:37,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1140/6000\n",
      "Fitnesses: ['-46.62', '-25.81', '-13.43', '-70.88']\n",
      "100 fitness avgs: ['-29.53', '-30.06', '-30.36', '-29.95']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████████████▍                                                                                             | 1159/6000 [19:20<2:41:13,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1160/6000\n",
      "Fitnesses: ['-18.11', '-60.28', '-40.04', '-36.03']\n",
      "100 fitness avgs: ['-30.15', '-30.87', '-30.52', '-29.64']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▊                                                                                             | 1179/6000 [20:09<3:16:40,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1180/6000\n",
      "Fitnesses: ['-71.76', '-12.21', '-5.56', '-11.76']\n",
      "100 fitness avgs: ['-30.85', '-30.21', '-29.73', '-29.83']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▏                                                                                            | 1199/6000 [20:57<3:21:39,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200/6000\n",
      "Fitnesses: ['-10.47', '-16.45', '-8.86', '-5.73']\n",
      "100 fitness avgs: ['-29.41', '-29.51', '-29.48', '-30.43']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▌                                                                                            | 1219/6000 [21:45<3:09:49,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1220/6000\n",
      "Fitnesses: ['-34.35', '-9.45', '-4.34', '-57.00']\n",
      "100 fitness avgs: ['-30.50', '-29.18', '-29.07', '-29.86']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This tutorial shows how to train an MATD3 agent on the simple speaker listener multi-particle environment.\n",
    "\n",
    "Authors: Michael (https://github.com/mikepratt1), Nickua (https://github.com/nicku-a)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #現在日時を取得\n",
    "    dt_now = datetime.datetime.now()\n",
    "    str_dt_now = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "    print(str_dt_now)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "\n",
    "    # Define the network configuration\n",
    "    # ネットワークコンフィグレーションの定義\n",
    "    NET_CONFIG = {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Actor hidden size\n",
    "    }\n",
    "\n",
    "    # Define the initial hyperparameters\n",
    "    # 初期ハイパーパラメータの定義\n",
    "    INIT_HP = {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MATD3\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "    }\n",
    "\n",
    "    # Define the simple speaker listener environment as a parallel environment\n",
    "    # シンプル・スピーカー・リスナー環境（並列）の定義\n",
    "    env = simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the multi-agent algo input arguments\n",
    "    # マルチ・エージェントアルゴへの入力引数の設定\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    \n",
    "    if False: # デバッグ出力\n",
    "        print(\"state_dim\", state_dim)\n",
    "        print(\"one_hot\", one_hot)\n",
    "        print( 'INIT_HP[\"DISCRETE_ACTIONS\"]', INIT_HP[\"DISCRETE_ACTIONS\"])\n",
    "        print( 'INIT_HP[\"MAX_ACTION\"]', INIT_HP[\"MAX_ACTION\"])\n",
    "        print( 'INIT_HP[\"MIN_ACTION\"]', INIT_HP[\"MIN_ACTION\"])\n",
    "    \n",
    "\n",
    "    # Not applicable to MPE environments, used when images are used for observations (Atari environments)\n",
    "    # MPE環境には適用されない。観測のために画像が使用される場合に使う（アタリ環境）\n",
    "    if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    # エージェントとエージェントIDの数を、ハイパーパラメータディクショナリの初期化のために加える\n",
    "    INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "    INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "    \n",
    "    if True: # デバッグ出力\n",
    "        print('state_dim', state_dim)\n",
    "        print('action_dim', action_dim)\n",
    "        print('one_hot', one_hot)\n",
    "        print('NET_CONFIG')\n",
    "        pprint.pprint(NET_CONFIG)\n",
    "        print('INIT_HP')\n",
    "        pprint.pprint(INIT_HP)\n",
    "        print('device', device)\n",
    "    \n",
    "    # Create a population ready for evolutionary hyper-parameter optimisation\n",
    "    # 進化的なハイパーパラメータ最適化のための母集団を作成する\n",
    "    population = initialPopulation(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Configure the multi-agent replay buffer\n",
    "    # マルチ・エージェント・リプレイバッファを設定する\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        INIT_HP[\"MEMORY_SIZE\"],\n",
    "        field_names=field_names,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Instantiate a tournament selection object (used for HPO)\n",
    "    # トーナメント選択オブジェクトのインスタンス化（HPOで使用）\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=2,  # Tournament selection size\n",
    "        elitism=True,  # Elitism in tournament selection\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "        evo_step=1,\n",
    "    )  # Evaluate using last N fitness scores\n",
    "\n",
    "    # Instantiate a mutations object (used for HPO)\n",
    "    # ミューテーション・オブジェクトのインスタンス化（HPOで使用）\n",
    "    mutations = Mutations(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        no_mutation=0.2,  # Probability of no mutation\n",
    "        architecture=0.2,  # Probability of architecture mutation\n",
    "        new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "        parameters=0.2,  # Probability of parameter mutation\n",
    "        activation=0,  # Probability of activation function mutation\n",
    "        rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "        rl_hp_selection=[\n",
    "            \"lr\",\n",
    "            \"learn_step\",\n",
    "            \"batch_size\",\n",
    "        ],  # RL hyperparams selected for mutation\n",
    "        mutation_sd=0.1,  # Mutation strength\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        arch=NET_CONFIG[\"arch\"],\n",
    "        rand_seed=2,#1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Define training loop parameters\n",
    "    # 学習ループ・パラメータを定義\n",
    "    max_episodes = 6000 #500  # Total episodes (default: 6000)\n",
    "    max_steps = 100 #25  # Maximum steps to take in each episode\n",
    "    epsilon = 1.0  # Starting epsilon value\n",
    "    eps_end = 0.1  # Final epsilon value\n",
    "    eps_decay = 0.995  # Epsilon decay\n",
    "    evo_epochs = 20  # Evolution frequency\n",
    "    evo_loop = 1  # Number of evaluation episodes\n",
    "    elite = population[0]  # Assign a placeholder \"elite\" agent\n",
    "\n",
    "    # Training loop\n",
    "    # 学習ループ\n",
    "    for idx_epi in trange(max_episodes):\n",
    "        \n",
    "        for agent in population:  # Loop through population\n",
    "            \n",
    "            state, info = env.reset()  # Reset environment at start of episode\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "                env_defined_actions = (\n",
    "                    info[\"env_defined_actions\"]\n",
    "                    if \"env_defined_actions\" in info.keys()\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "                # Get next action from agent\n",
    "                # エージェントから次の行動を取得する\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                if agent.discrete_actions:\n",
    "                    action = discrete_action\n",
    "                else:\n",
    "                    action = cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(\n",
    "                    action\n",
    "                )  # Act in environment\n",
    "\n",
    "                # Image processing if necessary for the environment\n",
    "                # 環境に応じた画像処理を行う\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                # Save experiences to replay buffer\n",
    "                # 経験をリプレイバッファに保存する\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                # Collect the reward\n",
    "                # 報酬を受け取る\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                # Learn according to learning frequency\n",
    "                # 学習周期に合わせて学習する\n",
    "                if (memory.counter % agent.learn_step == 0) and (\n",
    "                    len(memory) >= agent.batch_size\n",
    "                ):\n",
    "                    experiences = memory.sample(\n",
    "                        agent.batch_size\n",
    "                    )  # Sample replay buffer\n",
    "                    agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "                # Update the state\n",
    "                # 状態を更新する\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                # Stop episode if any agents have terminated\n",
    "                # いずれかのエージェントが終了したならば、エピソードを停止する\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            # Save the total episode reward\n",
    "            # エピソードの合計報酬を保存する\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        # Update epsilon for exploration\n",
    "        # 探索用のイプシロンを更新\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        # Now evolve population if necessary\n",
    "        # 必要であれば、母集団を進化させる\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            # Evaluate population\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in population\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in population]}'\n",
    "            )\n",
    "\n",
    "            # Tournament selection and population mutation\n",
    "            # トーナメント選択と母集団の変異\n",
    "            elite, population = tournament.select(population)\n",
    "            population = mutations.mutation(population)\n",
    "\n",
    "    # Save the trained algorithm\n",
    "    # 学習アルゴリズムを保存する\n",
    "    path = \"./models/MATD3\"\n",
    "    #path = \"./\"+str_dt_now+\"/models/MATD3\"\n",
    "    filename = \"MATD3_trained_agent.pt\"\n",
    "    filename = f\"MATD3_trained_agent_{str_dt_now}.pt\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_path = os.path.join(path, filename)\n",
    "    elite.saveCheckpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4757375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "\n",
    "# Define function to return image\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm into the MADDPG object\n",
    "    #path = \"./models/MATD3/MATD3_trained_agent.pt\"\n",
    "    #path = \"./\"+str_dt_now+\"/models/MATD3/MATD3_trained_agent.pt\"\n",
    "    path = f\"./models/MATD3/{filename}\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 10  # Number of episodes to test agent on\n",
    "    max_steps = 25  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "    \n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()\n",
    "\n",
    "    # Save the gif to specified path\n",
    "    gif_path = \"./videos/\"\n",
    "    print(os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"))\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        #os.path.join(\"./videos/\", \"speaker_listener.gif\"), frames, duration=10\n",
    "        os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d75d986-de09-4449-ab01-394fcb3ce289",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Load the saved algorithm\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#path = f\"./models/MATD3/{filename}\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/MATD3/MATD3_trained_agent_20231225-1958.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmatd3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of episodes for testing\u001b[39;00m\n\u001b[1;32m     76\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m  \u001b[38;5;66;03m# Max steps per episode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/agilerl/algorithms/matd3.py:1103\u001b[0m, in \u001b[0;36mMATD3.loadCheckpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadCheckpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads saved agent properties and network weights from checkpoint.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m    :param path: Location to load checkpoint from\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m    :type path: string\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1103\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_config \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet_config\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/dill/_dill.py:442\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_main_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore:\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;66;03m# point obj class to main\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-tensorflow/lib/python3.11/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.mpe import simple_speaker_listener_v4\n",
    "from PIL import Image, ImageDraw\n",
    "from agilerl.algorithms.matd3 import MATD3\n",
    "\n",
    "# Define function to return image with episode number label\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = simple_speaker_listener_v4.parallel_env(\n",
    "        continuous_actions=True, render_mode=\"rgb_array\"\n",
    "    )\n",
    "    env.reset()\n",
    "\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    matd3 = MATD3(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the saved algorithm\n",
    "    #path = f\"./models/MATD3/{filename}\"\n",
    "    path = \"./models/MATD3/MATD3_trained_agent_20231225-1958.pt\"\n",
    "    matd3.loadCheckpoint(path)\n",
    "\n",
    "    episodes = 10  # Number of episodes for testing\n",
    "    max_steps = 25  # Max steps per episode\n",
    "\n",
    "    rewards = []  # Total episodic rewards\n",
    "    frames = []  # Frames for visualization\n",
    "    indi_agent_rewards = {agent_id: [] for agent_id in agent_ids}  # Individual agent rewards\n",
    "\n",
    "    speaker_0_rewards = []  # Rewards for speaker_0\n",
    "    listener_0_rewards = []  # Rewards for listener_0\n",
    "\n",
    "    # Test loop\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # ... [existing action selection and environment interaction code] ...\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = matd3.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if matd3.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # ... [existing environment step and reward processing code] ...\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "            \n",
    "        # Store rewards for each agent\n",
    "        Episodic_rewards.append(indi_agent_rewards['speaker_0'][-1])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save the visualization\n",
    "    gif_path = \"./videos/\"\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(os.path.join(gif_path, f\"speaker_listener_{str_dt_now}.gif\"), frames, duration=10)\n",
    "\n",
    "    # Plotting rewards\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(Episodic_rewards, label='Episodic_rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Episodic Rewards for Speaker and Listener Agents')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642b609-f427-4afe-afae-34029d8fa55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
